{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddedb90",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this project is to Spending based on the other available customer information. This file contains data about whether or not different consumers made a purchase in response to a test mailing of a certain catalog and, in case of a purchase, how much money each consumer spent.\n",
    "\n",
    "We will be using 9 different classification algorithms shown below - \n",
    "\n",
    "1. K-Nearest Neighbors\n",
    "2. Decision Trees\n",
    "3. Naive Bayes\n",
    "4. Linear Regression\n",
    "5. Support Vector Machines\n",
    "6. Elastic Net Regression\n",
    "7. Random Forests\n",
    "8. Gradient Boosting\n",
    "9. XGBoost\n",
    "\n",
    "We will also try this in two different parts - \n",
    "1. Part A - Without using the information in the purchase indicator\n",
    "2. Part B - Using the information in the purchase indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bd579",
   "metadata": {},
   "source": [
    "### Important Packages\n",
    "\n",
    "We will be using the scikit-learn package in python to train models for our prediction problem. We will also use matplotlib to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f13c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af07e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US</th>\n",
       "      <th>source_a</th>\n",
       "      <th>source_c</th>\n",
       "      <th>source_b</th>\n",
       "      <th>source_d</th>\n",
       "      <th>source_e</th>\n",
       "      <th>source_m</th>\n",
       "      <th>source_o</th>\n",
       "      <th>source_h</th>\n",
       "      <th>source_r</th>\n",
       "      <th>...</th>\n",
       "      <th>source_x</th>\n",
       "      <th>source_w</th>\n",
       "      <th>Freq</th>\n",
       "      <th>last_update_days_ago</th>\n",
       "      <th>1st_update_days_ago</th>\n",
       "      <th>Web order</th>\n",
       "      <th>Gender=male</th>\n",
       "      <th>Address_is_res</th>\n",
       "      <th>Purchase</th>\n",
       "      <th>Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3662</td>\n",
       "      <td>3662</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>127.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2900</td>\n",
       "      <td>2900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3883</td>\n",
       "      <td>3914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>829</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   US  source_a  source_c  source_b  source_d  source_e  source_m  source_o  \\\n",
       "0   1         0         0         1         0         0         0         0   \n",
       "1   1         0         0         0         0         1         0         0   \n",
       "2   1         0         0         0         0         0         0         0   \n",
       "3   1         0         1         0         0         0         0         0   \n",
       "4   1         0         1         0         0         0         0         0   \n",
       "\n",
       "   source_h  source_r  ...  source_x  source_w  Freq  last_update_days_ago  \\\n",
       "0         0         0  ...         0         0     2                  3662   \n",
       "1         0         0  ...         0         0     0                  2900   \n",
       "2         0         0  ...         0         0     2                  3883   \n",
       "3         0         0  ...         0         0     1                   829   \n",
       "4         0         0  ...         0         0     1                   869   \n",
       "\n",
       "   1st_update_days_ago  Web order  Gender=male  Address_is_res  Purchase  \\\n",
       "0                 3662          1            0               1         1   \n",
       "1                 2900          1            1               0         0   \n",
       "2                 3914          0            0               0         1   \n",
       "3                  829          0            1               0         0   \n",
       "4                  869          0            0               0         0   \n",
       "\n",
       "   Spending  \n",
       "0    127.87  \n",
       "1      0.00  \n",
       "2    127.48  \n",
       "3      0.00  \n",
       "4      0.00  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cleaning the data\n",
    "\n",
    "data = pd.read_csv('HW3.csv')\n",
    "\n",
    "data = data.drop(labels = 'sequence_number', axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a72eac",
   "metadata": {},
   "source": [
    "### Part A - Split into train, test and validation\n",
    "\n",
    "To avoid overfitting, we will split our data into 2 parts - train and test.\n",
    "\n",
    "\n",
    "We will use the validation set to select the best model from all the different hyperparameters and then finally compare the results of all the different models on the testing dataset to decide which model works the best.\n",
    "\n",
    "The split that I have chosen - \n",
    "\n",
    "- Train = 70% \n",
    "- Test = 30%\n",
    "\n",
    "We will first attempt to target part 1 of this exercise - without using the purchase indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd9daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset for two parts of the question\n",
    "\n",
    "data_part2 = data[data['Purchase']==1]\n",
    "\n",
    "## Training and testing \n",
    "X = data.drop(labels = ['Purchase','Spending'], axis = 1)\n",
    "y = data['Spending']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24e34f",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will use nested cross-validation to select the best model out of the all choices we have.\n",
    "\n",
    "We follow the steps below for each model - \n",
    "\n",
    "1. We select the list of parameters we want to optimize over for each model and put it into a dictionary \n",
    "2. Set up the inner cross-validation object by using this parameter grid\n",
    "3. Set up the outer cross-validation object.\n",
    "5. Create a grid search object using the inner cv object. This will be used to find the best parameter for each outer fold.\n",
    "4. Get the cross validation score using the grid search object as the estimator and the outer_cv as the cross validation parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb5c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting hyperparameters\n",
    "\n",
    "### Create model objects\n",
    "\n",
    "knn = make_pipeline(MinMaxScaler(), neighbors.KNeighborsRegressor())\n",
    "dt = tree.DecisionTreeRegressor()\n",
    "lm = make_pipeline(StandardScaler(), LinearRegression())\n",
    "svm_regressor = make_pipeline(MinMaxScaler(), SVR())\n",
    "gm_regressor = make_pipeline(MinMaxScaler(), GradientBoostingRegressor())\n",
    "enet_regressor = make_pipeline(MinMaxScaler(), ElasticNet())\n",
    "rf_regressor = make_pipeline(MinMaxScaler(), RandomForestRegressor())\n",
    "xgb_regressor = make_pipeline(MinMaxScaler(), XGBRegressor(verbosity = 0))\n",
    "\n",
    "###### Create Parameter List\n",
    "\n",
    "## KNN\n",
    "k_range = list(range(1,10))\n",
    "knn_params = dict(kneighborsregressor__n_neighbors = k_range, \n",
    "                  kneighborsregressor__weights = ['uniform','distance'], \n",
    "                  kneighborsregressor__p = [1,2,3])\n",
    "\n",
    "## Decision Tree\n",
    "depth_range = list(range(1,10))\n",
    "min_samples_range = list(range(2,10))\n",
    "impurity_decrease_range = list(np.linspace(0.1,0.5,5))\n",
    "\n",
    "dt_params = dict(criterion = ['squared_error','absolute_error'], \n",
    "                 splitter = ['best','random'],\n",
    "                 max_depth = depth_range, \n",
    "                 max_features = ['auto','sqrt','log2',None], \n",
    "                 random_state = [456],\n",
    "                 min_impurity_decrease = impurity_decrease_range)\n",
    "\n",
    "## Linear Regression and Elastic Net\n",
    "\n",
    "\n",
    "l1r_range = [0,0.5,1]\n",
    "\n",
    "enet_params = dict(elasticnet__alpha = [0.1,1,10],\n",
    "                   elasticnet__l1_ratio = l1r_range)\n",
    "\n",
    "## Support Vector Machine\n",
    "\n",
    "c_range = [0.1, 1, 10]\n",
    "\n",
    "svm_params = {\"svr__C\": c_range,\n",
    "              \"svr__degree\": list(range(1,5)),\n",
    "              \"svr__kernel\": ['rbf'],\n",
    "              \"svr__max_iter\": [10000]\n",
    "            }\n",
    "\n",
    "## Random Forest\n",
    "rf_params = {\n",
    "    \"randomforestregressor__n_estimators\": [100,200,500],\n",
    "    \"randomforestregressor__max_depth\": list(range(5,10)),\n",
    "    \"randomforestregressor__bootstrap\": [True,False]}\n",
    "\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "gb_params = {\n",
    "    \"gradientboostingregressor__n_estimators\": [100,500,1000,2000],\n",
    "    \"gradientboostingregressor__learning_rate\": [0.001,0.01,0.1],\n",
    "    \"gradientboostingregressor__loss\": ['squared_error']}\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "xgb_params = {\n",
    "    \"xgbregressor__n_estimators\": [100,200,500],\n",
    "    \"xgbregressor__max_depth\": depth_range,\n",
    "    \"xgbregressor__learning_rate\": [0.01,0.1]}\n",
    "\n",
    "## Neural Networks\n",
    "## https://stackoverflow.com/questions/44132652/keras-how-to-perform-a-prediction-using-kerasregressor\n",
    "def create_model(activation='relu', nb_hidden=10, nb_hidden_2 = 10, nb_hidden_3 = 10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_hidden, input_dim=22, activation=activation))\n",
    "    model.add(Dense(nb_hidden_2, activation=activation))\n",
    "    model.add(Dense(nb_hidden_3, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "nb_hiddens = np.array([100, 200])\n",
    "nb_hiddens_2 = np.array([10,20])\n",
    "nb_hiddens_3 = np.array([5,10])\n",
    "\n",
    "param_grid = dict(kerasregressor__activation=activations,\n",
    "                  kerasregressor__nb_hidden=nb_hiddens, \n",
    "                  kerasregressor__nb_hidden_2 = nb_hiddens_2, \n",
    "                  kerasregressor__nb_hidden_3 = nb_hiddens_3)\n",
    "\n",
    "model = make_pipeline(StandardScaler(), \n",
    "                      KerasRegressor(build_fn=create_model,\n",
    "                                     epochs=2,\n",
    "                                     batch_size=256,\n",
    "                                     verbose=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35bc2823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000288929E55E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028892AA9D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "##### Set up inner and outerCV loops\n",
    "\n",
    "inner_cv = KFold(n_splits = 5, shuffle = True, random_state = 456)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 456)\n",
    "\n",
    "### Use metric\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "\n",
    "### Create Grid Search estimators\n",
    "\n",
    "knn_gs = GridSearchCV(estimator = knn, param_grid = knn_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "dt_gs = GridSearchCV(estimator = dt, param_grid = dt_params, \n",
    "                     scoring = scoring, cv = inner_cv)\n",
    "svm_gs = GridSearchCV(estimator = svm_regressor, param_grid = svm_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "enet_gs = GridSearchCV(estimator = enet_regressor, param_grid = enet_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "rf_gs = GridSearchCV(estimator = rf_regressor, param_grid = rf_params, \n",
    "                    scoring = scoring, cv = inner_cv)\n",
    "gbm_gs = GridSearchCV(estimator = gm_regressor, param_grid = gb_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "xgb_gs = GridSearchCV(estimator = xgb_regressor, param_grid = xgb_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "nn_gs = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                  cv= inner_cv, scoring=scoring)\n",
    "\n",
    "### performing Nested cross validation using the inner and outer loops\n",
    "dt_score = cross_val_score(estimator = dt_gs, X = X_train, y = y_train, \n",
    "                           cv = outer_cv, scoring = scoring)\n",
    "knn_score = cross_val_score(estimator = knn_gs, X = X_train, y = y_train, \n",
    "                            cv = outer_cv, scoring = scoring)\n",
    "enet_score = cross_val_score(estimator = enet_gs, X = X_train, y = y_train, \n",
    "                              cv = outer_cv, scoring = scoring)\n",
    "svm_score = cross_val_score(estimator = svm_gs, X = X_train, y = y_train, \n",
    "                            cv = outer_cv, scoring = scoring)\n",
    "\n",
    "lm_score = cross_val_score(estimator = lm, X = X_train, y = y_train, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "rf_score = cross_val_score(estimator = rf_gs, X = X_train, y = y_train, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "gbm_score = cross_val_score(estimator = gbm_gs, X = X_train, y = y_train, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "xgb_score = cross_val_score(estimator = xgb_gs, X = X_train, y = y_train, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "nn_score = cross_val_score(estimator = nn_gs, X = X_train, y = y_train, cv = outer_cv,\n",
    "                           scoring = scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac61be3",
   "metadata": {},
   "source": [
    "### Part A - Results using mean squared error\n",
    "\n",
    "It looks like the xgboost algorithm does the best on our training data with an mean squared error of 15137.43. We will use this model to fit our data. The model performances are listed below - \n",
    "\n",
    "1. knn = 29025.11 Â± 3949.38\n",
    "2. dt = 18257.52 Â± 4874.32\n",
    "3. lm = 15919.46 Â± 3457.37\n",
    "4. enet = 32206.54 Â± 3480.73\n",
    "5. svm = 32206.54 Â± 4180.3\n",
    "6. random forest = 16015.65 Â± 3704.79\n",
    "7. gradient boost = 15597.9 Â± 3865.95\n",
    "8. xgboost = 15137.43 Â± 3762.25\n",
    "9. nn = 45442.36 Â± 4196.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01d7675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn = 29025.11 Â± 3949.38\n",
      "dt = 18257.52 Â± 4874.32\n",
      "lm = 15919.46 Â± 3457.37\n",
      "enet = 32206.54 Â± 3480.73\n",
      "svm = 32206.54 Â± 4180.3\n",
      "random forest = 16015.65 Â± 3704.79\n",
      "gradient boost = 15597.9 Â± 3865.95\n",
      "xgboost = 15137.43 Â± 3762.25\n",
      "nn = 45442.36 Â± 4196.29\n"
     ]
    }
   ],
   "source": [
    "print(\"knn = \" + str(np.round(-knn_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \" \n",
    "      + str(np.round(knn_score.std(),2)))\n",
    "print(\"dt = \" + str(np.round(-dt_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \" \n",
    "      + str(np.round(dt_score.std(),2)))\n",
    "print(\"lm = \" + str(np.round(-lm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(lm_score.std(),2)))\n",
    "print(\"enet = \" + str(np.round(-svm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(enet_score.std(),2)))\n",
    "print(\"svm = \" + str(np.round(-svm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(svm_score.std(),2)))\n",
    "print(\"random forest = \" + str(np.round(-rf_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(rf_score.std(),2)))\n",
    "print(\"gradient boost = \" + str(np.round(-gbm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(gbm_score.std(),2)))\n",
    "print(\"xgboost = \" + str(np.round(-xgb_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(xgb_score.std(),2)))\n",
    "print(\"nn = \" + str(np.round(-nn_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(nn_score.std(),2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893f4eb",
   "metadata": {},
   "source": [
    "## Re-training and evaluation\n",
    "\n",
    "Now we re-train our XGBoost model on the entire training dataset and evaluate it on the testing set. It looks like our model performs well on the testing set as well and gives us a mean squared error of 11236.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b38a312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11236.089168417564"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Since Xgboost performs the best, we will use that for our models ahead\n",
    "X = data.drop(labels = ['Purchase','Spending'], axis = 1)\n",
    "y = data['Spending']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)\n",
    "\n",
    "final_model = xgb_gs.fit(X_train,y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952e498",
   "metadata": {},
   "source": [
    "## Part B - Using purchase indicator\n",
    "\n",
    "Using the information in the purchase indicator, we repeat the steps of the Nested Cross Validation, but this time only with the data with purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16a2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and testing \n",
    "X_1 = data_part2.drop(labels = ['Spending'], axis = 1)\n",
    "y_1 = data_part2['Spending']\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, train_size = 0.8)\n",
    "\n",
    "##### Set up inner and outerCV loops\n",
    "\n",
    "inner_cv = KFold(n_splits = 5, shuffle = True, random_state = 456)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 456)\n",
    "\n",
    "### Use metric\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "\n",
    "### Create Grid Search estimators\n",
    "\n",
    "knn_gs = GridSearchCV(estimator = knn, param_grid = knn_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "dt_gs = GridSearchCV(estimator = dt, param_grid = dt_params, \n",
    "                     scoring = scoring, cv = inner_cv)\n",
    "svm_gs = GridSearchCV(estimator = svm_regressor, param_grid = svm_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "enet_gs = GridSearchCV(estimator = enet_regressor, param_grid = enet_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "rf_gs = GridSearchCV(estimator = rf_regressor, param_grid = rf_params, \n",
    "                    scoring = scoring, cv = inner_cv)\n",
    "gbm_gs = GridSearchCV(estimator = gm_regressor, param_grid = gb_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "xgb_gs = GridSearchCV(estimator = xgb_regressor, param_grid = xgb_params, \n",
    "                      scoring = scoring, cv = inner_cv)\n",
    "nn_gs = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                  cv= inner_cv, scoring=scoring)\n",
    "\n",
    "### performing Nested cross validation using the inner and outer loops\n",
    "dt_score = cross_val_score(estimator = dt_gs, X = X_train_1, y = y_train_1, \n",
    "                           cv = outer_cv, scoring = scoring)\n",
    "knn_score = cross_val_score(estimator = knn_gs, X = X_train_1, y = y_train_1, \n",
    "                            cv = outer_cv, scoring = scoring)\n",
    "enet_score = cross_val_score(estimator = enet_gs, X = X_train_1, y = y_train_1, \n",
    "                              cv = outer_cv, scoring = scoring)\n",
    "svm_score = cross_val_score(estimator = svm_gs, X = X_train_1, y = y_train_1, \n",
    "                            cv = outer_cv, scoring = scoring)\n",
    "\n",
    "lm_score = cross_val_score(estimator = lm, X = X_train_1, y = y_train_1, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "rf_score = cross_val_score(estimator = rf_gs, X = X_train_1, y = y_train_1, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "gbm_score = cross_val_score(estimator = gbm_gs, X = X_train_1, y = y_train_1, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "xgb_score = cross_val_score(estimator = xgb_gs, X = X_train_1, y = y_train_1, cv = outer_cv,\n",
    "                           scoring = scoring)\n",
    "nn_score = cross_val_score(estimator = nn_gs, X = X_train_1, y = y_train_1, cv = outer_cv,\n",
    "                           scoring = scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843110d",
   "metadata": {},
   "source": [
    "### Part A - Results using mean squared error\n",
    "\n",
    "It looks like the xgboost algorithm does the best on our training data with an mean squared error of 26259. We will use this model to fit our data. The model performances are listed below - \n",
    "\n",
    "1. knn = 42827.45 Â± 11978.29\n",
    "2. dt = 30231.82 Â± 7507.77\n",
    "3. lm = 26474.49 Â± 7750.66\n",
    "4. enet = 47477.56 Â± 7746.15\n",
    "5. svm = 47477.56 Â± 12520.09\n",
    "6. random forest = 25682.17 Â± 7412.36\n",
    "7. gradient boost = 25156.34 Â± 7755.74\n",
    "8. xgboost = 26259.16 Â± 7251.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a505a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn = 42827.45 Â± 11978.29\n",
      "dt = 30231.82 Â± 7507.77\n",
      "lm = 26474.49 Â± 7750.66\n",
      "enet = 47477.56 Â± 7746.15\n",
      "svm = 47477.56 Â± 12520.09\n",
      "random forest = 25682.17 Â± 7412.36\n",
      "gradient boost = 25156.34 Â± 7755.74\n",
      "xgboost = 26259.16 Â± 7251.22\n",
      "nn = nan Â± nan\n"
     ]
    }
   ],
   "source": [
    "print(\"knn = \" + str(np.round(-knn_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \" \n",
    "      + str(np.round(knn_score.std(),2)))\n",
    "print(\"dt = \" + str(np.round(-dt_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \" \n",
    "      + str(np.round(dt_score.std(),2)))\n",
    "print(\"lm = \" + str(np.round(-lm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(lm_score.std(),2)))\n",
    "print(\"enet = \" + str(np.round(-svm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(enet_score.std(),2)))\n",
    "print(\"svm = \" + str(np.round(-svm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(svm_score.std(),2)))\n",
    "print(\"random forest = \" + str(np.round(-rf_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(rf_score.std(),2)))\n",
    "print(\"gradient boost = \" + str(np.round(-gbm_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(gbm_score.std(),2)))\n",
    "print(\"xgboost = \" + str(np.round(-xgb_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(xgb_score.std(),2)))\n",
    "print(\"nn = \" + str(np.round(-nn_score.mean(),2)) + \" \" + u\"\\u00B1\" + \" \"\n",
    "      + str(np.round(nn_score.std(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee8122",
   "metadata": {},
   "source": [
    "## Comparing the results of Part A and B\n",
    "\n",
    "1. Overall - it seems like models in part A do better than models in part B\n",
    "2. If we look at observations that have purchase = 1, and then calculate the mse for only those observations, we see that models in part B do better\n",
    "\n",
    "One reason for this is in Part A, models learn from zero spend observations and tend to underpredict on the purchase = 1 observations. In Part B, this does not happen as there are no zero spend observations which skew the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a6fa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31501.254804623717"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_1 = xgb_gs.fit(X_train_1,y_train_1)\n",
    "\n",
    "y_pred_1 = final_model_1.predict(X_test_1)\n",
    "mean_squared_error(y_test_1, y_pred_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
